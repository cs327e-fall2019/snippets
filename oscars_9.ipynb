{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/beam_venv_dir/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported\n",
      "  pipeline.replace_all(_get_transform_overrides(pipeline.options))\n",
      "WARNING:root:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "WARNING:root:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "# DoFn with multiple outputs\n",
    "class ActorActressCountFn(beam.DoFn):\n",
    "    \n",
    "  OUTPUT_TAG_ACTOR_COUNT = 'tag_actor_count'\n",
    "  OUTPUT_TAG_ACTRESS_COUNT = 'tag_actress_count'\n",
    "  \n",
    "  def process(self, element):\n",
    "    \n",
    "    from apache_beam import pvalue\n",
    "    \n",
    "    values = element.strip().split('\\t')\n",
    "    year = values[0]\n",
    "    category = values[1]\n",
    "    winner = values[2]\n",
    "    entity = values[3]\n",
    "\n",
    "    if 'ACTOR' in category:\n",
    "        yield pvalue.TaggedOutput(self.OUTPUT_TAG_ACTOR_COUNT, (entity, 1))  \n",
    "        \n",
    "    if 'ACTRESS' in category:\n",
    "        yield pvalue.TaggedOutput(self.OUTPUT_TAG_ACTRESS_COUNT, (entity, 1))  \n",
    "  \n",
    "# DoFn with single output\n",
    "class SumNominationsFn(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "     name, counts_obj = element\n",
    "     counts = list(counts_obj)\n",
    "     sum_counts = len(counts)\n",
    "     return [(name, sum_counts)]  \n",
    "\n",
    "# DoFn with single output\n",
    "class MakeBQRecordFn(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "     name, total_nominations = element\n",
    "     record = {name, total_nominations} \n",
    "     return [record]   \n",
    "    \n",
    "PROJECT_ID = '<YOUR PROJECT ID>'\n",
    "\n",
    "# Project ID is needed for BigQuery data source, even for local execution.\n",
    "options = {\n",
    "    'project': PROJECT_ID\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "# Create a Pipeline using a local runner for execution.\n",
    "with beam.Pipeline('DirectRunner', options=opts) as p:\n",
    "\n",
    "    # create a PCollection from the file contents.\n",
    "    in_pcoll = p | 'Read File' >> ReadFromText('oscars_input.tsv')\n",
    "\n",
    "    # apply a ParDo to the PCollection \n",
    "    out_pcoll = in_pcoll | 'Extract Actor and Actress' >> beam.ParDo(ActorActressCountFn()).with_outputs(\n",
    "                                                          ActorActressCountFn.OUTPUT_TAG_ACTOR_COUNT,\n",
    "                                                          ActorActressCountFn.OUTPUT_TAG_ACTRESS_COUNT)\n",
    "                                                          \n",
    "    actor_pcoll = out_pcoll[ActorActressCountFn.OUTPUT_TAG_ACTOR_COUNT]\n",
    "    actress_pcoll = out_pcoll[ActorActressCountFn.OUTPUT_TAG_ACTRESS_COUNT]\n",
    "\n",
    "    # write PCollections to files\n",
    "    actor_pcoll | 'Write Actor File 1' >> WriteToText('actor_output.txt')\n",
    "    actress_pcoll | 'Write Actress File 1' >> WriteToText('actress_output.txt')\n",
    "    \n",
    "    # apply GroupByKey \n",
    "    grouped_actor_pcoll = actor_pcoll | 'Group by Actor' >> beam.GroupByKey()\n",
    "    grouped_actress_pcoll = actress_pcoll | 'Group by Actress' >> beam.GroupByKey()\n",
    "    \n",
    "    # write PCollections to files\n",
    "    grouped_actor_pcoll | 'Write Actor File 2' >> WriteToText('grouped_actor_output.txt')\n",
    "    grouped_actress_pcoll | 'Write Actress File 2' >> WriteToText('grouped_actress_output.txt')\n",
    "\n",
    "    # apply ParDo with single DoFn to both PCollections\n",
    "    summed_actor_pcoll = grouped_actor_pcoll | 'Sum up Actor Nominations' >> beam.ParDo(SumNominationsFn())\n",
    "    summed_actress_pcoll = grouped_actress_pcoll | 'Sum up Actress Nominations' >> beam.ParDo(SumNominationsFn())\n",
    "    \n",
    "    # write PCollections to files\n",
    "    summed_actor_pcoll | 'Write Actor File 3' >> WriteToText('summed_actor_output.txt')\n",
    "    summed_actress_pcoll | 'Write Actress File 3' >> WriteToText('summed_actress_output.txt')\n",
    "    \n",
    "    bq_actor_pcoll = summed_actor_pcoll | 'Make BQ Actor' >> beam.ParDo(MakeBQRecordFn())\n",
    "    bq_actress_pcoll = summed_actress_pcoll | 'Make BQ Actress' >> beam.ParDo(MakeBQRecordFn())\n",
    "    \n",
    "    # write PCollections to files\n",
    "    bq_actor_pcoll | 'Write Actor File 4' >> WriteToText('bq_actor_output.txt')\n",
    "    bq_actress_pcoll | 'Write Actress File 4' >> WriteToText('bq_actress_output.txt')\n",
    "    \n",
    "    actor_table_name = PROJECT_ID + ':oscars.Actor_Nomination'\n",
    "    actress_table_name = PROJECT_ID + ':oscars.Actress_Nomination'\n",
    "    table_schema = 'name:STRING,nominations:INTEGER'\n",
    "        \n",
    "    # write PCollections to BQ tables\n",
    "    bq_actor_pcoll | 'Write Actor Table' >> beam.io.Write(beam.io.BigQuerySink(actor_table_name, \n",
    "                                                    schema=table_schema,  \n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))\n",
    "                                                    \n",
    "    bq_actress_pcoll | 'Write Actress Table' >> beam.io.Write(beam.io.BigQuerySink(actress_table_name, \n",
    "                                                    schema=table_schema,  \n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
