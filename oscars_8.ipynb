{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import os, datetime\n",
    "from __future__ import absolute_import\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '<YOUR PROJECT ID>'\n",
    "BUCKET = 'gs://<YOUR BUCKET>'\n",
    "DIR_PATH_IN = BUCKET + '/input/' \n",
    "DIR_PATH_OUT = BUCKET + '/output/' + datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S') + '/'\n",
    "\n",
    "# DoFn with multiple outputs\n",
    "class ActorActressCountFn(beam.DoFn):\n",
    "    \n",
    "  OUTPUT_TAG_ACTOR_COUNT = 'tag_actor_count'\n",
    "  OUTPUT_TAG_ACTRESS_COUNT = 'tag_actress_count'\n",
    "  \n",
    "  def process(self, element):\n",
    "        \n",
    "    from apache_beam import pvalue\n",
    "    \n",
    "    values = element.strip().split('\\t')\n",
    "    year = values[0]\n",
    "    category = values[1]\n",
    "    winner = values[2]\n",
    "    entity = values[3]\n",
    "\n",
    "    if 'ACTOR' in category:\n",
    "        yield pvalue.TaggedOutput(self.OUTPUT_TAG_ACTOR_COUNT, (entity, 1))  \n",
    "        \n",
    "    if 'ACTRESS' in category:\n",
    "        yield pvalue.TaggedOutput(self.OUTPUT_TAG_ACTRESS_COUNT, (entity, 1))  \n",
    "\n",
    "# DoFn with single output\n",
    "class SumNominationsFn(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "     name, counts = element  \n",
    "     total_counts = len(counts)\n",
    "     return [(name, total_counts)] \n",
    " \n",
    "# run pipeline on Dataflow \n",
    "options = {\n",
    "    'runner': 'DataflowRunner',\n",
    "    'job_name': 'oscars-8',\n",
    "    'region': 'us-central1',\n",
    "    'project': PROJECT_ID,\n",
    "    'temp_location': BUCKET + '/temp',\n",
    "    'staging_location': BUCKET + '/staging',\n",
    "    'machine_type': 'n1-standard-1', # machine types listed here: https://cloud.google.com/compute/docs/machine-types\n",
    "    'num_workers': 1\n",
    "}\n",
    "opts = PipelineOptions(flags=[], **options)\n",
    "\n",
    "with beam.Pipeline('DataflowRunner', options=opts) as p:\n",
    "\n",
    "    # create PCollection from the file contents\n",
    "    #in_pcoll = p | 'Read File' >> ReadFromText(DIR_PATH_IN + 'oscars_input.tsv')\n",
    "    in_pcoll = p | 'Read File' >> ReadFromText(DIR_PATH_IN + 'oscars_input.tsv')\n",
    "\n",
    "    # apply ParDo with tagged outputs \n",
    "    out_pcoll = in_pcoll | 'Extract Actor and Actress' >> beam.ParDo(ActorActressCountFn()).with_outputs(\n",
    "                                                          ActorActressCountFn.OUTPUT_TAG_ACTOR_COUNT,\n",
    "                                                          ActorActressCountFn.OUTPUT_TAG_ACTRESS_COUNT)\n",
    "                                                          \n",
    "    actor_pcoll = out_pcoll[ActorActressCountFn.OUTPUT_TAG_ACTOR_COUNT]\n",
    "    actress_pcoll = out_pcoll[ActorActressCountFn.OUTPUT_TAG_ACTRESS_COUNT]\n",
    "\n",
    "    # write PCollections to files\n",
    "    actor_pcoll | 'Write Actor File 1' >> WriteToText(DIR_PATH_OUT + 'actor_output.txt')\n",
    "    actress_pcoll | 'Write Actress File 1' >> WriteToText(DIR_PATH_OUT + 'actress_output.txt')\n",
    "    \n",
    "    # apply GroupByKey \n",
    "    grouped_actor_pcoll = actor_pcoll | 'Group by Actor' >> beam.GroupByKey()\n",
    "    grouped_actress_pcoll = actress_pcoll | 'Group by Actress' >> beam.GroupByKey()\n",
    "    \n",
    "    # write PCollections to files\n",
    "    grouped_actor_pcoll | 'Write Actor File 2' >> WriteToText(DIR_PATH_OUT + 'grouped_actor_output.txt')\n",
    "    grouped_actress_pcoll | 'Write Actress File 2' >> WriteToText(DIR_PATH_OUT + 'grouped_actress_output.txt')\n",
    "\n",
    "    # apply ParDo with single DoFn to both PCollections\n",
    "    summed_actor_pcoll = grouped_actor_pcoll | 'Sum up Actor Nominations' >> beam.ParDo(SumNominationsFn())\n",
    "    summed_actress_pcoll = grouped_actress_pcoll | 'Sum up Actress Nominations' >> beam.ParDo(SumNominationsFn())\n",
    "    \n",
    "    # write PCollections to files\n",
    "    summed_actor_pcoll | 'Write Actor File 3' >> WriteToText(DIR_PATH_OUT + 'summed_actor_output.txt')\n",
    "    summed_actress_pcoll | 'Write Actress File 3' >> WriteToText(DIR_PATH_OUT + 'summed_actress_output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
