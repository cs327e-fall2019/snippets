{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime, logging, traceback\n",
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/beam_venv_dir/local/lib/python2.7/site-packages/apache_beam/io/gcp/gcsio.py:176: DeprecationWarning: object() takes no parameters\n",
      "  super(GcsIO, cls).__new__(cls, storage_client))\n"
     ]
    }
   ],
   "source": [
    "# DoFn with multiple outputs\n",
    "class ActorActressCountFn(beam.DoFn):\n",
    "    \n",
    "  OUTPUT_TAG_ACTOR_COUNT = 'tag_actor_count'\n",
    "  OUTPUT_TAG_ACTRESS_COUNT = 'tag_actress_count'\n",
    "  \n",
    "  def process(self, element):\n",
    "    \n",
    "    from apache_beam import pvalue\n",
    "    \n",
    "    values = element.strip().split('\\t')\n",
    "    year = values[0]\n",
    "    category = values[1]\n",
    "    winner = values[2]\n",
    "    entity = values[3].title()\n",
    "\n",
    "    if 'ACTOR' in category:\n",
    "        yield pvalue.TaggedOutput(self.OUTPUT_TAG_ACTOR_COUNT, (entity, 1))  \n",
    "        \n",
    "    if 'ACTRESS' in category:\n",
    "        yield pvalue.TaggedOutput(self.OUTPUT_TAG_ACTRESS_COUNT, (entity, 1))  \n",
    "  \n",
    "# DoFn with single output\n",
    "class SumNominationsFn(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "     name, counts_obj = element\n",
    "     counts = list(counts_obj)\n",
    "     sum_counts = str(len(counts))\n",
    "     return [(name, sum_counts)]  \n",
    "\n",
    "# DoFn with single output\n",
    "class MakeBQRecordFn(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "     \n",
    "     name, total_nominations = element\n",
    "     record = {'name': name, 'nominations' : total_nominations}\n",
    "     return [record]   \n",
    "\n",
    " \n",
    "PROJECT_ID = '<YOUR PROJECT ID>'\n",
    "BUCKET = 'gs://<YOUR BUCKET>'\n",
    "DIR_PATH_IN = BUCKET + '/input/' \n",
    "DIR_PATH_OUT = BUCKET + '/output/' + datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S') + '/'\n",
    "\n",
    "# run pipeline on Dataflow \n",
    "options = {\n",
    "    'runner': 'DataflowRunner',\n",
    "    'job_name': 'nomination-count-10',\n",
    "    'project': PROJECT_ID,\n",
    "    'temp_location': BUCKET + '/temp',\n",
    "    'staging_location': BUCKET + '/staging',\n",
    "    'machine_type': 'n1-standard-16', # machine types listed here: https://cloud.google.com/compute/docs/machine-types\n",
    "    'num_workers': 1\n",
    "}\n",
    "\n",
    "opts = PipelineOptions(flags=[], **options)\n",
    "\n",
    "with beam.Pipeline('DataflowRunner', options=opts) as p:\n",
    "\n",
    "    # create PCollection from the file contents\n",
    "    in_pcoll = p | 'Read File' >> ReadFromText(DIR_PATH_IN + 'oscars_input.tsv')\n",
    "\n",
    "    # apply a ParDo to the PCollection \n",
    "    out_pcoll = in_pcoll | 'Extract Actor and Actress' >> beam.ParDo(ActorActressCountFn()).with_outputs(\n",
    "                                                          ActorActressCountFn.OUTPUT_TAG_ACTOR_COUNT,\n",
    "                                                          ActorActressCountFn.OUTPUT_TAG_ACTRESS_COUNT)\n",
    "                                                          \n",
    "    actor_pcoll = out_pcoll[ActorActressCountFn.OUTPUT_TAG_ACTOR_COUNT]\n",
    "    actress_pcoll = out_pcoll[ActorActressCountFn.OUTPUT_TAG_ACTRESS_COUNT]\n",
    "\n",
    "    # write PCollections to files\n",
    "    actor_pcoll | 'Write Actor File 1' >> WriteToText(DIR_PATH_OUT + 'actor_output.txt')\n",
    "    actress_pcoll | 'Write Actress File 1' >> WriteToText(DIR_PATH_OUT + 'actress_output.txt')\n",
    "    \n",
    "    # apply GroupByKey \n",
    "    grouped_actor_pcoll = actor_pcoll | 'Group by Actor' >> beam.GroupByKey()\n",
    "    grouped_actress_pcoll = actress_pcoll | 'Group by Actress' >> beam.GroupByKey()\n",
    "    \n",
    "    # write PCollections to files\n",
    "    grouped_actor_pcoll | 'Write Actor File 2' >> WriteToText(DIR_PATH_OUT + 'grouped_actor_output.txt')\n",
    "    grouped_actress_pcoll | 'Write Actress File 2' >> WriteToText(DIR_PATH_OUT + 'grouped_actress_output.txt')\n",
    "\n",
    "    # apply ParDo \n",
    "    summed_actor_pcoll = grouped_actor_pcoll | 'Sum up Actor Nominations' >> beam.ParDo(SumNominationsFn())\n",
    "    summed_actress_pcoll = grouped_actress_pcoll | 'Sum up Actress Nominations' >> beam.ParDo(SumNominationsFn())\n",
    "    \n",
    "    # write PCollections to files\n",
    "    summed_actor_pcoll | 'Write Actor File 3' >> WriteToText(DIR_PATH_OUT + 'summed_actor_output.txt')\n",
    "    summed_actress_pcoll | 'Write Actress File 3' >> WriteToText(DIR_PATH_OUT + 'summed_actress_output.txt')\n",
    "    \n",
    "    # apply ParDo  \n",
    "    bq_actor_pcoll = actor_pcoll | 'Make BQ Actor' >> beam.ParDo(MakeBQRecordFn())\n",
    "    bq_actress_pcoll = actress_pcoll | 'Make BQ Actress' >> beam.ParDo(MakeBQRecordFn())\n",
    "    \n",
    "    # write PCollections to files\n",
    "    bq_actor_pcoll | 'Write Actor File 4' >> WriteToText(DIR_PATH_OUT + 'bq_actor_output.txt')\n",
    "    bq_actress_pcoll | 'Write Actress File 4' >> WriteToText(DIR_PATH_OUT + 'bq_actress_output.txt')\n",
    "    \n",
    "    actor_table_name = PROJECT_ID + ':oscars.Actor_Nominations'\n",
    "    actress_table_name = PROJECT_ID + ':oscars.Actress_Nominations'\n",
    "    table_schema = 'name:STRING,nominations:INTEGER'\n",
    "\n",
    "    # write PCollections to BQ tables\n",
    "    bq_actor_pcoll | 'Write Actor Table' >> beam.io.Write(beam.io.BigQuerySink(actor_table_name, \n",
    "                                                    schema=table_schema,  \n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))\n",
    "                                                    \n",
    "    bq_actress_pcoll | 'Write Actress Table' >> beam.io.Write(beam.io.BigQuerySink(actress_table_name, \n",
    "                                                    schema=table_schema,  \n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
